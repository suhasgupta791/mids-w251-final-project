{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc685aea70>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ijson\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "# from apex import amp\n",
    "import shutil\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "SEED = 7219\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"/Users/suhasgupta/w251/mids-w251-final-project/data/nlp.cs.princeton.edu/SARC/2.0/pol/\"\n",
    "# COMMENTS_FILE=DATA_DIR+\"comments.json\"\n",
    "# print(COMMENTS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(COMMENTS_FILE, 'r') as f:\n",
    "#     comments = next(ijson.items(f, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments['7uxqr']\n",
    "# len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dict(itertools.islice(comments.items(),1000000))\n",
    "# df = pd.DataFrame.from_dict(d).T\n",
    "# df.head()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "# df['len'] = df['text'].apply(lambda x: len(x.split(\" \")))\n",
    "# import string\n",
    "# for i in string.punctuation:\n",
    "#     df['text'] = df['text'].apply(lambda x: x.replace(i, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['sign'] = df['score'].apply(lambda x: 1 if x >= 0 else -1)\n",
    "\n",
    "# # Import the sentiment table for emoticons\n",
    "# VADER_LEXICON = \"/Users/suhasgupta/w251/mids-w251-final-project/models/Tranformer_based/vader_lexicon.txt\"\n",
    "# sen = pd.read_csv(VADER_LEXICON, \n",
    "#                    sep='\\t',\n",
    "#                    usecols=[0, 1], \n",
    "#                    header=None, \n",
    "#                    names=['token', 'polarity'],\n",
    "#                    index_col='token'\n",
    "#                   )\n",
    "# sen.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy_format = (\n",
    "#     df['text']\n",
    "#     .str.split(expand=True)\n",
    "#     .stack()\n",
    "#     .reset_index(level=1)\n",
    "#     .rename(columns={'level_1': 'num', 0: 'word'})\n",
    "# )\n",
    "# tidy_format.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['polarity'] = (\n",
    "#     tidy_format\n",
    "#     .merge(sen, how='left', left_on='word', right_index=True)\n",
    "#     .reset_index()\n",
    "#     .loc[:, ['index', 'polarity']]\n",
    "#     .groupby('index')\n",
    "#     .sum()\n",
    "#     .fillna(0)\n",
    "# )\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('sign').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(DATA_DIR+'processed_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniqueId</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>date</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>ups</th>\n",
       "      <th>len</th>\n",
       "      <th>sign</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7uxqr</td>\n",
       "      <td>Fishbum</td>\n",
       "      <td>1233788424</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "      <td>nancyt pelosi messes up 500 million jobs lost ...</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7vewt</td>\n",
       "      <td>jdl2003</td>\n",
       "      <td>1233940024</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>252</td>\n",
       "      <td>1733</td>\n",
       "      <td>politics</td>\n",
       "      <td>netflix ceo please raise my taxes</td>\n",
       "      <td>1985</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7vq9q</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1234070398</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "      <td>the six million dead jews of world war one</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c07jfvv</td>\n",
       "      <td>Erobern</td>\n",
       "      <td>1234070581</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>politics</td>\n",
       "      <td>oh right both wars were just jewish conspiraci...</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7w0as</td>\n",
       "      <td>fangolo</td>\n",
       "      <td>1234194112</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>167</td>\n",
       "      <td>891</td>\n",
       "      <td>politics</td>\n",
       "      <td>gop says it is necessary to spend my tax dolla...</td>\n",
       "      <td>1058</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c07kq5w</td>\n",
       "      <td>_pi</td>\n",
       "      <td>1234237732</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>do not question the hive mind</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c07myx2</td>\n",
       "      <td>jk1150</td>\n",
       "      <td>1234486080</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>politics</td>\n",
       "      <td>yup all republicans think exactly the same way</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7xdys</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1234625565</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>politics</td>\n",
       "      <td>wsj begins the jeb bush campaign for 2016</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c07o37s</td>\n",
       "      <td>Mastrmind</td>\n",
       "      <td>1234642047</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>politics</td>\n",
       "      <td>good luck with that</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7xvzm</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1234825236</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>112</td>\n",
       "      <td>129</td>\n",
       "      <td>politics</td>\n",
       "      <td>breaking a crucial campaign promise obama defe...</td>\n",
       "      <td>241</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  UniqueId     author  created_utc     date  downs  score subreddit  \\\n",
       "0    7uxqr    Fishbum   1233788424  2009-02      4      0  politics   \n",
       "1    7vewt    jdl2003   1233940024  2009-02    252   1733  politics   \n",
       "2    7vq9q  [deleted]   1234070398  2009-02     23      0  politics   \n",
       "3  c07jfvv    Erobern   1234070581  2009-02      0      6  politics   \n",
       "4    7w0as    fangolo   1234194112  2009-02    167    891  politics   \n",
       "5  c07kq5w        _pi   1234237732  2009-02      0      1  politics   \n",
       "6  c07myx2     jk1150   1234486080  2009-02      0      4  politics   \n",
       "7    7xdys  [deleted]   1234625565  2009-02     10     14  politics   \n",
       "8  c07o37s  Mastrmind   1234642047  2009-02      0      2  politics   \n",
       "9    7xvzm  [deleted]   1234825236  2009-02    112    129  politics   \n",
       "\n",
       "                                                text   ups  len  sign  \\\n",
       "0  nancyt pelosi messes up 500 million jobs lost ...     2   19     1   \n",
       "1                  netflix ceo please raise my taxes  1985    6     1   \n",
       "2         the six million dead jews of world war one    20    9     1   \n",
       "3  oh right both wars were just jewish conspiraci...     6   14     1   \n",
       "4  gop says it is necessary to spend my tax dolla...  1058   26     1   \n",
       "5                      do not question the hive mind     1    6     1   \n",
       "6     yup all republicans think exactly the same way     4    8     1   \n",
       "7          wsj begins the jeb bush campaign for 2016    24    8     1   \n",
       "8                                good luck with that     2    4     1   \n",
       "9  breaking a crucial campaign promise obama defe...   241   17     1   \n",
       "\n",
       "   polarity  \n",
       "0       3.6  \n",
       "1       1.3  \n",
       "2      -6.2  \n",
       "3      -2.6  \n",
       "4      -1.8  \n",
       "5       0.0  \n",
       "6       0.0  \n",
       "7       0.0  \n",
       "8       3.9  \n",
       "9       4.8  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_read_from_file = pd.read_csv(data_path+'train_df.csv')\n",
    "# df_read_from_file = df_read_from_file.rename(columns={\"Unnamed: 0\": \"UniqueId\"})\n",
    "# df_read_from_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# sample_seed = 1234 \n",
    "# train_df, valid_df = train_test_split(df_read_from_file, test_size=0.2,random_state=sample_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19664    obama supports new bid to ban assault weapons ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# text_sample = train_df.head(1).text\n",
    "# print(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.to_csv(train_df,data_path+'processed_train_df.csv')\n",
    "# pd.DataFrame.to_csv(valid_df,data_path+'processed_valid_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_length = len(train_df)\n",
    "# valid_length = len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_lines(example,record_len,max_seq_len):\n",
    "# #     tokenizer = self._tokenizer\n",
    "# #     max_seq_length = self._MAX_SEQUENCE_LENGTH-2\n",
    "#     all_tokens = []\n",
    "#     longer = 0\n",
    "#     for text in tqdm_notebook(example):\n",
    "#         tokens_a = tokenizer.tokenize(text)\n",
    "#         if len(tokens_a)>max_seq_length:\n",
    "#             tokens_a = tokens_a[:max_seq_length]\n",
    "#             longer += 1\n",
    "#         one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "#         all_tokens.append(one_token)\n",
    "#     print(longer)\n",
    "#     return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len = 128\n",
    "# train_df_tokenized = convert_lines(train_df[\"text\"].fillna(\"DUMMY_VALUE\"),train_length,max_seq_len)\n",
    "# train_df_tokenized_tensor = torch.tensor(train_df_tokenized)\n",
    "# valid_df_tokenized = convert_lines(valid_df[\"text\"].fillna(\"DUMMY_VALUE\"),train_length,max_seq_len)\n",
    "# valid_df_tokenized_tensor = torch.tensor(valid_df_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_df_tokenized_tensor,DATA_DIR+'/train_df_tokenized_tensor.pt')\n",
    "# torch.save(valid_df_tokenized_tensor,DATA_DIR+'/valid_df_tokenized_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = torch.load(DATA_DIR+'/train_df_tokenized_tensor.pt')\n",
    "# valid_data = torch.load(DATA_DIR+'/valid_df_tokenized_tensor.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
