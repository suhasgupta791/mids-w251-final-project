{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faf7098ea70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ijson\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "# from apex import amp\n",
    "import shutil\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "SEED = 7219\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformers specific packages\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import  BertForSequenceClassification, BertForTokenClassification\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device and empty cache \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for model training and inference\n",
    "class Bert_Model():\n",
    "    def __init__(self,train_df,test_df,bert_model_name,bert_model_path,\n",
    "                tokenizer,\n",
    "                max_seq_length=128,seed=1234):\n",
    "        \n",
    "        if max_seq_length > tokenizer.max_model_input_sizes[bert_model_name]:\n",
    "            print(\"Max sequence length specified > 512!!... resetting to 128\")\n",
    "            print(\"If you don't want this then set max_seq_length to <= 512\")\n",
    "            self._MAX_SEQUENCE_LENGTH = 128\n",
    "        else:\n",
    "            self._MAX_SEQUENCE_LENGTH = max_seq_length\n",
    "        self._SEED = seed\n",
    "        self._WORK_DIR = \"/Users/suhasgupta/w251/mids-w251-final-project/models/Tranformer_based/workingdir/\"\n",
    "        self._bert_model_path=bert_model_path\n",
    "        self._bert_model_name=bert_model_name\n",
    "        self._train_data=train_df\n",
    "        self._test_size=test_df\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self,text_array):\n",
    "        ''' Returns tokenized IDs'''\n",
    "#         all_tokens=torch.zeros((len(text_array),self._MAX_SEQUENCE_LENGTH))\n",
    "        all_tokens=[]\n",
    "        for i,text in enumerate(tqdm_notebook(text_array)):\n",
    "            tokens = torch.tensor(\n",
    "                            tokenizer.encode(\n",
    "                                   text, \n",
    "                                   add_special_tokens=True,\n",
    "                                   max_length=self._MAX_SEQUENCE_LENGTH,\n",
    "                                   pad_to_max_length=True))\n",
    "#             all_tokens[i]=tokens\n",
    "            all_tokens.append(tokens)\n",
    "        return all_tokens\n",
    "\n",
    "    def initialize_model_for_training(self,dataset_len,EPOCHS=1,model_seed=21000,lr=2e-5,batch_size=32,\n",
    "                                      accumulation_steps=2):\n",
    "        # Setup model parameters\n",
    "        np.random.seed(model_seed)\n",
    "        torch.manual_seed(model_seed)\n",
    "        torch.cuda.manual_seed(model_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "        model = BertForSequenceClassification.from_pretrained(self._bert_model_path,cache_dir=None,num_labels=2)\n",
    "        model.zero_grad()\n",
    "        model = model.to(device)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        num_train_optimization_steps = int(EPOCHS*dataset_len/batch_size/accumulation_steps)\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=1000,num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n",
    "#         model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "#         torch.nn.utils.clip_grad_norm_(optimizer_grouped_parameters,1.0)\n",
    "        model=model.train()\n",
    "        return model,optimizer,scheduler,EPOCHS\n",
    "\n",
    "    def run_training(self,model,train_dataLoader,optimizer,scheduler,EPOCHS=1,batch_size=32,accumulation_steps=2):\n",
    "        tq = tqdm_notebook(range(EPOCHS))\n",
    "        for epoch in tq:\n",
    "#             train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            avg_loss = 0.\n",
    "            avg_accuracy = 0.\n",
    "            lossf=None\n",
    "            tk0 = tqdm_notebook(enumerate(train_dataLoader),total=len(train_dataLoader),leave=False)\n",
    "            optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n",
    "            for i,(x_batch, y_batch) in tk0:\n",
    "#                 y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)  \n",
    "                outputs = model(x_batch.to(device), labels=y_batch.to(device))\n",
    "                loss, y_pred = outputs\n",
    "#                 loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n",
    "#                 with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#                     scaled_loss.backward()\n",
    "                loss.backward()\n",
    "                if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                    optimizer.step()                            # Now we can do an optimizer step\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                if lossf:\n",
    "                    lossf = 0.98*lossf+0.02*loss.item()\n",
    "                else:\n",
    "                    lossf = loss.item()\n",
    "                tk0.set_postfix(loss = lossf)\n",
    "                avg_loss += loss.item() / len(train_dataLoader)\n",
    "#                 avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:])>0.5) == (y_batch[:]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
    "            tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "            return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        self._dataset = [[train_data_tokenized[i],train_labels.values[i]] for i in range(0,len(train_data_tokenized))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self._dataset[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205665 51417 64666\n"
     ]
    }
   ],
   "source": [
    "# Define constants and paths\n",
    "\n",
    "seed = 7843\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "# Convert TF checkpoint to pytorch chckpoint and then use as input to class object\n",
    "bert_model_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/BERT/uncased_L-12_H-768_A-12-pytorch/\"\n",
    "# data_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/nlp.cs.princeton.edu/SARC/2.0/pol/\"\n",
    "data_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/nlp.cs.princeton.edu/SARC/2.0/files/\"\n",
    "work_dir = \"/Users/suhasgupta/w251/mids-w251-final-project/models/Tranformer_based/workingdir/\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, cache_dir=None,do_lower_case=True)\n",
    "max_seq_len = 64\n",
    "\n",
    "# Load and check the dataset from files on disk \n",
    "train_file_name = data_path+'balanced_train.csv'\n",
    "test_file_name  = data_path+'balanced_test.csv'\n",
    "\n",
    "all_train_df = pd.read_csv(train_file_name)\n",
    "test_df = pd.read_csv(test_file_name)\n",
    "\n",
    "# Create a train, valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_train_df, test_size=0.2,random_state=seed)\n",
    "\n",
    "train_data   = train_df.text.fillna(\"DUMMY_VALUE\")\n",
    "train_labels = train_df.label\n",
    "valid_data  = valid_df.text.fillna(\"DUMMY_VALUE\")\n",
    "valid_labels = valid_df.label\n",
    "test_data  = test_df.text.fillna(\"DUMMY_VALUE\")\n",
    "test_labels = test_df.label\n",
    "\n",
    "train_size,valid_size,test_size = len(train_df),len(valid_df),len(test_df)\n",
    "print(train_size,valid_size,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object \n",
    "bert_model1=Bert_Model(train_df=train_df,\n",
    "                      test_df=test_df,\n",
    "                      bert_model_name=bert_model_name,\n",
    "                      bert_model_path=bert_model_path,\n",
    "                      tokenizer=tokenizer,\n",
    "                      max_seq_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee9619e7d224c169c37c7b51f826d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=205665), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962230aae5ed414796673ce4cd703a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51417), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bba1cc362134dd7b42c5bc176437a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64666), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "train_data_tokenized = bert_model1.tokenize(train_data)\n",
    "valid_data_tokenized = bert_model1.tokenize(valid_data)\n",
    "test_data_tokenized = bert_model1.tokenize(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(train_data_tokenized,train_labels)\n",
    "valid_dataset = CreateDataset(valid_data_tokenized,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  101,  2106,  2017,  9530,  8873, 27390,  2063,  9779, 11140,  3512,\n",
       "         19528,  3370,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]), 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[tensor([  101,  2106,  2017,  9530,  8873, 27390,  2063,  9779, 11140,  3512,\n",
       "         19528,  3370,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]), 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]\n",
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of batches: 6427\n",
      "Generated number of batches:6427\n"
     ]
    }
   ],
   "source": [
    "params = {'batch_size': 32,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "max_epochs = 1\n",
    "print(\"Expected number of batches:\", int(len(train_data_tokenized)/params['batch_size']))\n",
    "train_dataLoader = torch.utils.data.DataLoader(train_dataset,batch_size=params['batch_size'],\n",
    "                                         shuffle=params['shuffle'],\n",
    "                                         num_workers=params['num_workers'],\n",
    "                                         pin_memory=False,drop_last=True)\n",
    "print(\"Generated number of batches:%d\" %len(train_dataLoader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model,optimizer,scheduler,EPOCHS = bert_model1.initialize_model_for_training(len(train_dataLoader),EPOCHS=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc71e419f72416189d1eeab5a9d977b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252aa7f842a3421e9d108daf852bf0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6427), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train \n",
    "bert_model1.run_training(model,train_dataLoader,\n",
    "                         optimizer=optimizer,scheduler=scheduler,EPOCHS=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
