{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 7219\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5879: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/suhasgupta/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Import transformers specific packages\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import  BertForSequenceClassification, BertForTokenClassification\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device and empty cache \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.empty_cache()\n",
    "    from apex import amp\n",
    "else:\n",
    "    device='cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for model training and inference\n",
    "class Bert_Model():\n",
    "    def __init__(self,train_df,test_df,bert_model_name,bert_model_path,\n",
    "                tokenizer,\n",
    "                max_seq_length=128,seed=1234):\n",
    "        \n",
    "        if max_seq_length > tokenizer.max_model_input_sizes[bert_model_name]:\n",
    "            print(\"Max sequence length specified > 512!!... resetting to 128\")\n",
    "            print(\"If you don't want this then set max_seq_length to <= 512\")\n",
    "            self._MAX_SEQUENCE_LENGTH = 128\n",
    "        else:\n",
    "            self._MAX_SEQUENCE_LENGTH = max_seq_length\n",
    "        self._SEED = seed\n",
    "        self._WORK_DIR = \"/root/models/Tranformer_based/workingdir/\"\n",
    "        self._bert_model_path=bert_model_path\n",
    "        self._bert_model_name=bert_model_name\n",
    "        self._train_data=train_df\n",
    "        self._test_size=test_df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._training_stats = []\n",
    "\n",
    "    def tokenize(self,text_array):\n",
    "        ''' Returns tokenized IDs and attention mask\n",
    "        The transformers encode_plus method returns the following:\n",
    "        {\n",
    "        input_ids: list[int],\n",
    "        token_type_ids: list[int] if return_token_type_ids is True (default)\n",
    "        attention_mask: list[int] if return_attention_mask is True (default)\n",
    "        overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "        num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "        special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "        }'''\n",
    "        all_tokens=[]\n",
    "        all_attention_mask=[]\n",
    "        for i,text in enumerate(tqdm_notebook(text_array)):\n",
    "            encoded = tokenizer.encode_plus(\n",
    "                           text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=self._MAX_SEQUENCE_LENGTH,\n",
    "                           pad_to_max_length=True)\n",
    "            tokens = torch.tensor(encoded['input_ids'])\n",
    "            attention_mask = torch.tensor(encoded['attention_mask'])\n",
    "            all_tokens.append(tokens)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "        return all_tokens,all_attention_mask\n",
    "     \n",
    "    def initialize_model_for_training(self,dataset_len,EPOCHS=1,model_seed=21000,lr=2e-5,batch_size=32,\n",
    "                                      accumulation_steps=2):\n",
    "        # Setup model parameters\n",
    "        np.random.seed(model_seed)\n",
    "        torch.manual_seed(model_seed)\n",
    "        torch.cuda.manual_seed(model_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "        model = BertForSequenceClassification.from_pretrained(self._bert_model_path,\n",
    "                                                              cache_dir=None,\n",
    "                                                              num_labels=2,\n",
    "                                                              output_attentions = False, \n",
    "                                                              output_hidden_states = False)\n",
    "        model.zero_grad()\n",
    "        model = model.to(device)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        num_train_optimization_steps = int(EPOCHS*dataset_len/batch_size/accumulation_steps)\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr,eps=1e-8,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n",
    "        if device==\"cuda\":\n",
    "            model, optimizer = amp.initialize(model,optimizer,opt_level=\"O1\",verbosity=0)\n",
    "        model=model.train()\n",
    "        return model,optimizer,scheduler,EPOCHS\n",
    "    \n",
    "    def run_training(self,model,train_dataLoader,valid_dataLoader,optimizer,scheduler,EPOCHS=1,batch_size=32,accumulation_steps=2,logdir=\"./logs\"):\n",
    "        # Data Structure for training statistics\n",
    "        training_stats=[]\n",
    "        validation_stats=[]\n",
    "        \n",
    "        tr_loss = 0.\n",
    "        tr_accuracy = 0.\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        tq = tqdm_notebook(range(EPOCHS),total=EPOCHS,leave=False)\n",
    "        global_step = 0\n",
    "        for epoch in tq:\n",
    "            print(\"--Training--\")\n",
    "            tk0 = tqdm_notebook(enumerate(train_dataLoader),total=len(train_dataLoader),leave=True)\n",
    "            for step,(x_batch,attn_mask,y_batch) in tk0:\n",
    "                outputs = model(x_batch.to(device), \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=attn_mask.to(device), \n",
    "                                labels=y_batch.to(device))\n",
    "                y_pred = outputs[1]\n",
    "                loss = outputs[0] / accumulation_steps\n",
    "                if device=='cuda':\n",
    "                    with amp.scale_loss(loss,optimizer) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                tk0.set_postfix(step=global_step+1,loss=loss.item()) # display running loss\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                acc = torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch>0.5).to(device)).to(torch.float) ).item()/len(train_dataLoader)\n",
    "                tr_accuracy += acc\n",
    "                if (step+1) % accumulation_steps == 0:          # Wait for several backward steps\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip the norm to 1.0 to prevent exploding gradients\n",
    "\n",
    "                    optimizer.step()                            # Now we can do an optimizer step\n",
    "                    scheduler.step()\n",
    "                    model.zero_grad()\n",
    "                    global_step+=1\n",
    "#                     optimizer.zero_grad()\n",
    "#                     print(\"Step Number:%d||Train Loss:%0.5f||Train Accuracy:%0.5f\" %(step+1,avg_loss,avg_accuracy))\n",
    "\n",
    "                    training_stats.append(\n",
    "                            {\n",
    "                                'step': global_step+1,\n",
    "                                'train_loss': tr_loss/(global_step+1),\n",
    "                                'train_acc': tr_accuracy/(global_step+1),\n",
    "                            })\n",
    "                # Write training stats to tensorboard\n",
    "                self.summaryWriter(\"train\",loss.item(),acc,global_step+1,logdir)\n",
    "            \n",
    "                # Run evaluation\n",
    "                if (global_step+1) % (4*accumulation_steps)==0:\n",
    "                    print(\"--I-- Running Validation\")\n",
    "                    eval_loss,eval_accuracy=self.run_eval(model,valid_dataLoader,global_step+1)\n",
    "#                     print(\"Step Number:%d||Valid Loss:%0.5f||Valid Accuracy:%0.5f\" %(step+1,eval_loss,eval_accuracy))\n",
    "                    validation_stats.append(\n",
    "                        {\n",
    "                            'step': global_step + 1,\n",
    "                            'valid_loss': eval_loss,\n",
    "                            'Training Accuracy': eval_accuracy,\n",
    "                        })\n",
    "                    # Write training stats to tensorboard\n",
    "                    self.summaryWriter(\"eval\",eval_loss,eval_accuracy,global_step+1,logdir)\n",
    "            \n",
    "            tq.set_postfix(train_loss=tr_loss/(global_step+1),train_accuracy=tr_accuracy/(global_step+1))\n",
    "        return model,training_stats,validation_stats\n",
    "    \n",
    "    def run_eval(self,model,valid_dataLoader,global_step):\n",
    "        avg_loss = 0.\n",
    "        eval_accuracy = 0.\n",
    "        eval_loss = 0.\n",
    "        nb_eval_steps = 0\n",
    "        lossf=None\n",
    "        tk0 = tqdm_notebook(enumerate(valid_dataLoader),total=len(valid_dataLoader),leave=True)\n",
    "        for step,(x_batch, attn_mask,y_batch) in tk0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(x_batch.to(device), \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=attn_mask.to(device), \n",
    "                                labels=y_batch.to(device))\n",
    "            loss, y_pred = outputs\n",
    "            y_pred = y_pred.detach().cpu().numpy()\n",
    "            label_ids = y_batch.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = self.flat_accuracy(y_pred, label_ids)\n",
    "            if lossf:\n",
    "                lossf = 0.98*lossf+0.02*loss.item()\n",
    "            else:\n",
    "                lossf = loss.item()\n",
    "            # Accumulate the total accuracy.\n",
    "            eval_loss += lossf/len(valid_dataLoader)\n",
    "            eval_accuracy += tmp_eval_accuracy/len(valid_dataLoader)\n",
    "            nb_eval_steps += 1\n",
    "        avg_loss = eval_loss/nb_eval_steps\n",
    "        avg_accuracy = eval_accuracy/nb_eval_steps\n",
    "        tk0.set_postfix(step=global_step,avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "        return avg_loss,avg_accuracy\n",
    "\n",
    "     # Function to calculate the accuracy of predictions vs labels\n",
    "    def flat_accuracy(self,preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    def summaryWriter(self,name,loss,acc,n_iter,logdir):\n",
    "        # Writer will output to ./runs/ directory by default\n",
    "        writer = SummaryWriter(logdir)\n",
    "        writer.add_scalar('Loss/'+name,loss,n_iter)\n",
    "        writer.add_scalar('Accuracy/'+name,acc,n_iter)\n",
    "        writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self,data,atten_mask,labels):\n",
    "        self._dataset = [[data[i],atten_mask[i],labels.values[i]] for i in range(0,len(data))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self._dataset[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/suhasgupta/w251/mids-w251-final-project/models/Tranformer_based\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7999 2000 999\n"
     ]
    }
   ],
   "source": [
    "# Define constants and paths\n",
    "\n",
    "seed = 7843\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "# Convert TF checkpoint to pytorch chckpoint and then use as input to class object\n",
    "bert_model_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/BERT/uncased_L-12_H-768_A-12-pytorch/\"\n",
    "# data_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/nlp.cs.princeton.edu/SARC/2.0/pol/\"\n",
    "data_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/nlp.cs.princeton.edu/SARC/2.0/files/\"\n",
    "work_dir = \"/Users/suhasgupta/w251/mids-w251-final-project/models/Tranformer_based/workingdir/\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, cache_dir=None,do_lower_case=True)\n",
    "max_seq_len = 128\n",
    "\n",
    "# Load and check the dataset from files on disk \n",
    "# train_file_name = data_path+'balanced_train.csv'\n",
    "# test_file_name  = data_path+'balanced_test.csv'\n",
    "\n",
    "train_file_name = data_path+'small_train.csv'\n",
    "test_file_name  = data_path+'small_test.csv'\n",
    "\n",
    "all_train_df = pd.read_csv(train_file_name)\n",
    "test_df = pd.read_csv(test_file_name)\n",
    "\n",
    "# Create a train, valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_train_df, test_size=0.2,random_state=seed)\n",
    "\n",
    "train_data   = train_df.text.fillna(\"DUMMY_VALUE\")\n",
    "train_labels = train_df.label\n",
    "valid_data  = valid_df.text.fillna(\"DUMMY_VALUE\")\n",
    "valid_labels = valid_df.label\n",
    "test_data  = test_df.text.fillna(\"DUMMY_VALUE\")\n",
    "test_labels = test_df.label\n",
    "\n",
    "train_size,valid_size,test_size = len(train_df),len(valid_df),len(test_df)\n",
    "print(train_size,valid_size,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object \n",
    "bert_model1=Bert_Model(train_df=train_df,\n",
    "                      test_df=test_df,\n",
    "                      bert_model_name=bert_model_name,\n",
    "                      bert_model_path=bert_model_path,\n",
    "                      tokenizer=tokenizer,\n",
    "                      max_seq_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Tokenizing--\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ca075770924f07b7a38015a5d2519c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba036cc3141412ca93c33b8d0d2852f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ea903b9a56420d88269aa798a9858c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "print(\"--Tokenizing--\")\n",
    "train_data_tokenized,train_attention_mask = bert_model1.tokenize(train_data)\n",
    "valid_data_tokenized,valid_attention_mask = bert_model1.tokenize(valid_data)\n",
    "test_data_tokenized,test_attention_mask = bert_model1.tokenize(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(train_data_tokenized,train_attention_mask,train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_dataset = CreateDataset(valid_data_tokenized,valid_attention_mask,valid_labels)\n",
    "valid_sampler = RandomSampler(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]\n",
    "# valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated number of training batches:62\n",
      "Generated number of validation batches:15\n",
      "Number of training steps: 62\n",
      "Number of validation steps: 3.1\n"
     ]
    }
   ],
   "source": [
    "# # Parameters for cloud GPU\n",
    "# train_params = {'batch_size': 48,\n",
    "#           'shuffle': False,\n",
    "#           'num_workers': 16}\n",
    "# valid_params = {'batch_size': 48,\n",
    "#           'shuffle': False,\n",
    "#           'num_workers': 16}\n",
    "\n",
    "# Parameters for Suhas's Mac\n",
    "train_params = {'batch_size': 128,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "valid_params = {'batch_size': 128,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "\n",
    "# # Parameters for local GPU\n",
    "# train_params = {'batch_size': 32,\n",
    "#           'shuffle': False,\n",
    "#           'num_workers': 16}\n",
    "# valid_params = {'batch_size': 8,\n",
    "#           'shuffle': False,\n",
    "#           'num_workers': 16}\n",
    "\n",
    "max_epochs = 1\n",
    "accumulation_steps=20\n",
    "\n",
    "# print(\"Expected number of batches:\", int(len(train_data_tokenized)/params['batch_size']))\n",
    "train_dataLoader = torch.utils.data.DataLoader(train_dataset,sampler=train_sampler,batch_size=train_params['batch_size'],\n",
    "                                         shuffle=train_params['shuffle'],\n",
    "                                         num_workers=train_params['num_workers'],\n",
    "                                         pin_memory=False,drop_last=True)\n",
    "valid_dataLoader = torch.utils.data.DataLoader(valid_dataset,sampler=valid_sampler,batch_size=valid_params['batch_size'],\n",
    "                                         shuffle=valid_params['shuffle'],\n",
    "                                         num_workers=valid_params['num_workers'],\n",
    "                                         pin_memory=False,drop_last=True)\n",
    "print(\"Generated number of training batches:%d\" %len(train_dataLoader))\n",
    "print(\"Generated number of validation batches:%d\" %len(valid_dataLoader))\n",
    "print(\"Number of training steps:\",len(train_dataLoader)*max_epochs)\n",
    "print(\"Number of validation steps:\",len(train_dataLoader)*max_epochs/accumulation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model,optimizer,scheduler,EPOCHS = bert_model1.initialize_model_for_training(len(train_dataLoader),\n",
    "                                                                             EPOCHS=max_epochs,accumulation_steps=accumulation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171bb022bcd44fcf807115456c9edd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Training--\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe0429b97854b368b721ca1c96639ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=62), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train \n",
    "\n",
    "start = time.time()\n",
    "trained_model,training_stats,validation_stats=\\\n",
    "        bert_model1.run_training(model,train_dataLoader,valid_dataLoader,\n",
    "                         optimizer=optimizer,scheduler=scheduler,EPOCHS=EPOCHS,accumulation_steps=accumulation_steps)\n",
    "\n",
    "print(\"Training Time:%0.5f seconds\" %(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
