{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5506: ResourceWarning: unclosed file <_io.TextIOWrapper name='/root/.keras/keras.json' mode='r' encoding='ANSI_X3.4-1968'>\n",
      "  _config = json.load(open(_config_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/onnx_tf/handlers/backend/ceil.py:10: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/onnx_tf/handlers/backend/depth_to_space.py:12: The name tf.depth_to_space is deprecated. Please use tf.compat.v1.depth_to_space instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/onnx_tf/handlers/backend/erf.py:9: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/onnx_tf/common/__init__.py:89: UserWarning: onnx_tf.common.get_outputs_names is deprecated. It will be removed in future release. Use TensorflowGraph.get_outputs_names instead.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/onnx_tf/handlers/backend/is_nan.py:9: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/onnx_tf/handlers/backend/log.py:10: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/onnx_tf/handlers/backend/upsample.py:15: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "from apex import amp\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 7219\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformers specific packages\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import  BertForSequenceClassification, BertForTokenClassification\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device and empty cache \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add tensorboard summaries for monitoring training \n",
    "- Add accuracy, F1 score, log loss metric calculations\n",
    "- Add architectural layers (freeze BERT layers, add new layers)\n",
    "- Create pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for model training and inference\n",
    "class Bert_Model():\n",
    "    def __init__(self,train_df,test_df,bert_model_name,bert_model_path,\n",
    "                tokenizer,\n",
    "                max_seq_length=128,seed=1234):\n",
    "        \n",
    "        if max_seq_length > tokenizer.max_model_input_sizes[bert_model_name]:\n",
    "            print(\"Max sequence length specified > 512!!... resetting to 128\")\n",
    "            print(\"If you don't want this then set max_seq_length to <= 512\")\n",
    "            self._MAX_SEQUENCE_LENGTH = 128\n",
    "        else:\n",
    "            self._MAX_SEQUENCE_LENGTH = max_seq_length\n",
    "        self._SEED = seed\n",
    "        self._WORK_DIR = \"/root/models/Tranformer_based/workingdir/\"\n",
    "        self._bert_model_path=bert_model_path\n",
    "        self._bert_model_name=bert_model_name\n",
    "        self._train_data=train_df\n",
    "        self._test_size=test_df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._training_stats = []\n",
    "\n",
    "    def tokenize(self,text_array):\n",
    "        ''' Returns tokenized IDs and attention mask\n",
    "        The transformers encode_plus method returns the following:\n",
    "        {\n",
    "        input_ids: list[int],\n",
    "        token_type_ids: list[int] if return_token_type_ids is True (default)\n",
    "        attention_mask: list[int] if return_attention_mask is True (default)\n",
    "        overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "        num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "        special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "        }'''\n",
    "        all_tokens=[]\n",
    "        all_attention_mask=[]\n",
    "        for i,text in enumerate(tqdm_notebook(text_array)):\n",
    "            encoded = tokenizer.encode_plus(\n",
    "                           text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=self._MAX_SEQUENCE_LENGTH,\n",
    "                           pad_to_max_length=True)\n",
    "            tokens = torch.tensor(encoded['input_ids'])\n",
    "            attention_mask = torch.tensor(encoded['attention_mask'])\n",
    "            all_tokens.append(tokens)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "        return all_tokens,all_attention_mask\n",
    "     \n",
    "    def initialize_model_for_training(self,dataset_len,EPOCHS=1,model_seed=21000,lr=2e-5,batch_size=32,\n",
    "                                      accumulation_steps=2):\n",
    "        # Setup model parameters\n",
    "        np.random.seed(model_seed)\n",
    "        torch.manual_seed(model_seed)\n",
    "        torch.cuda.manual_seed(model_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "        model = BertForSequenceClassification.from_pretrained(self._bert_model_path,\n",
    "                                                              cache_dir=None,\n",
    "                                                              num_labels=2,\n",
    "                                                              output_attentions = False, \n",
    "                                                              output_hidden_states = False)\n",
    "        model.zero_grad()\n",
    "        model = model.to(device)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        num_train_optimization_steps = int(EPOCHS*dataset_len/batch_size/accumulation_steps)\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr,eps=1e-8,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n",
    "        model, optimizer = amp.initialize(model,optimizer,opt_level=\"O1\",verbosity=0)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        model = model.train()\n",
    "        return model,optimizer,scheduler,criterion,EPOCHS\n",
    "    \n",
    "    def run_training(self,model,train_dataLoader,valid_dataLoader,optimizer,scheduler,criterion,EPOCHS=1,batch_size=32,accumulation_steps=2,logdir='./logs'):\n",
    "        # Data Structure for training statistics\n",
    "        training_stats=[]\n",
    "        validation_stats=[]\n",
    "        \n",
    "        tr_loss = 0.\n",
    "        tr_accuracy = 0.\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        tq = tqdm_notebook(range(EPOCHS),total=EPOCHS,leave=False)\n",
    "        global_step = 0\n",
    "        for epoch in tq:\n",
    "            print(\"--Training--\")\n",
    "            tk0 = tqdm_notebook(enumerate(train_dataLoader),total=len(train_dataLoader),leave=True)\n",
    "            for step,(x_batch,attn_mask,y_batch) in tk0:\n",
    "                outputs = model(x_batch.to(device), \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=attn_mask.to(device), \n",
    "                                labels=y_batch.to(device))\n",
    "                y_pred = outputs[1]\n",
    "                loss = criterion(y_pred,y_batch.to(device)) / accumulation_steps\n",
    "                with amp.scale_loss(loss,optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "#                     loss.backward()\n",
    "                tk0.set_postfix(step=global_step+1,loss=loss.item()) # display running loss\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                acc = torch.mean(((torch.sigmoid(y_pred[:,1])>0.5) == (y_batch>0.5).to(device)).to(torch.float)).item()/len(train_dataLoader)\n",
    "                tr_accuracy += acc\n",
    "                if (step+1) % accumulation_steps == 0:       # Wait for several backward steps\n",
    "                    print(step+1)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip the norm to 1.0 to prevent exploding gradients\n",
    "                    optimizer.step()                            # Now we can do an optimizer step\n",
    "                    scheduler.step()\n",
    "                    model.zero_grad()\n",
    "                    global_step+=1\n",
    "#                     optimizer.zero_grad()\n",
    "#                     print(\"Step Number:%d||Train Loss:%0.5f||Train Accuracy:%0.5f\" %(step+1,avg_loss,avg_accuracy))\n",
    "                    training_stats.append(\n",
    "                            {\n",
    "                                'step': global_step,\n",
    "                                'train_loss': tr_loss/(global_step),\n",
    "                                'train_acc': tr_accuracy/(global_step),\n",
    "                            })\n",
    "                    # Write training stats to tensorboard\n",
    "                    self.summaryWriter(\"train\",loss.item(),acc,global_step,logdir)      \n",
    "                # Run evaluation\n",
    "                if (step+1) % (4*accumulation_steps)==0:\n",
    "                    print(\"--I-- Running Validation\")\n",
    "                    eval_loss,eval_accuracy=self.run_eval(model,valid_dataLoader,global_step)\n",
    "#                     print(\"Step Number:%d||Valid Loss:%0.5f||Valid Accuracy:%0.5f\" %(step+1,eval_loss,eval_accuracy))\n",
    "                    validation_stats.append(\n",
    "                        {\n",
    "                            'step': global_step,\n",
    "                            'valid_loss': eval_loss,\n",
    "                            'valid_acc': eval_accuracy,\n",
    "                        })\n",
    "                    # Write evaluation stats to tensorboard\n",
    "                    self.summaryWriter(\"eval\",eval_loss,eval_accuracy,global_step+1,logdir)\n",
    "            tq.set_postfix(train_loss=tr_loss/global_step,train_accuracy=tr_accuracy/global_step)\n",
    "        return model,training_stats,validation_stats\n",
    "    \n",
    "    def run_eval(self,model,valid_dataLoader,global_step):\n",
    "        avg_loss = 0.\n",
    "        eval_accuracy = 0.\n",
    "        eval_loss = 0.\n",
    "        nb_eval_steps = 0\n",
    "        lossf=None\n",
    "        tk0 = tqdm_notebook(enumerate(valid_dataLoader),total=len(valid_dataLoader),leave=True)\n",
    "        for step,(x_batch, attn_mask,y_batch) in tk0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(x_batch.to(device), \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=attn_mask.to(device), \n",
    "                                labels=y_batch.to(device))\n",
    "            loss, y_pred = outputs\n",
    "            loss = criterion(y_pred,y_batch.to(device))\n",
    "            y_pred = y_pred.detach().cpu().numpy()\n",
    "            label_ids = y_batch.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = self.flat_accuracy(y_pred, label_ids)\n",
    "            if lossf:\n",
    "                lossf = 0.98*lossf+0.02*loss.item()\n",
    "            else:\n",
    "                lossf = loss.item()\n",
    "            # Accumulate the total accuracy.\n",
    "            eval_loss += lossf/len(valid_dataLoader)\n",
    "            eval_accuracy += tmp_eval_accuracy/len(valid_dataLoader)\n",
    "            nb_eval_steps += 1      \n",
    "        avg_loss = eval_loss/nb_eval_steps\n",
    "        avg_accuracy = eval_accuracy/nb_eval_steps\n",
    "        tk0.set_postfix(step=global_step,avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "        return avg_loss,avg_accuracy\n",
    "\n",
    "     # Function to calculate the accuracy of predictions vs labels\n",
    "    def flat_accuracy(self,preds,labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    def summaryWriter(self,name,loss,acc,n_iter,logdir):\n",
    "        # Writer will output to ./runs/ directory by default\n",
    "        writer = SummaryWriter(logdir)\n",
    "        writer.add_scalar('Loss/'+name,loss,n_iter)\n",
    "        writer.add_scalar('Accuracy/'+name,acc,n_iter)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self,data,atten_mask,labels):\n",
    "        self._dataset = [[data[i],atten_mask[i],labels.values[i]] for i in range(0,len(data))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self._dataset[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205665 51417 64666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Define constants and paths\n",
    "\n",
    "seed = 7843\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "# Convert TF checkpoint to pytorch chckpoint and then use as input to class object\n",
    "bert_model_path = \"/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/\"\n",
    "# data_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/nlp.cs.princeton.edu/SARC/2.0/pol/\"\n",
    "data_path = \"/root/data/nlp.cs.princeton.edu/SARC/2.0/files/\"\n",
    "work_dir = \"/root/models/Tranformer_based/workingdir/\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, cache_dir=None,do_lower_case=True)\n",
    "max_seq_len = 128\n",
    "\n",
    "# Load and check the dataset from files on disk \n",
    "train_file_name = data_path+'balanced_train.csv'\n",
    "test_file_name  = data_path+'balanced_test.csv'\n",
    "\n",
    "# train_file_name = data_path+'small_train.csv'\n",
    "# test_file_name  = data_path+'small_test.csv'\n",
    "\n",
    "all_train_df = pd.read_csv(train_file_name)\n",
    "test_df = pd.read_csv(test_file_name)\n",
    "\n",
    "# Create a train, valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_train_df, test_size=0.2,random_state=seed)\n",
    "\n",
    "train_data   = train_df.text.fillna(\"DUMMY_VALUE\")\n",
    "train_labels = train_df.label\n",
    "valid_data  = valid_df.text.fillna(\"DUMMY_VALUE\")\n",
    "valid_labels = valid_df.label\n",
    "test_data  = test_df.text.fillna(\"DUMMY_VALUE\")\n",
    "test_labels = test_df.label\n",
    "\n",
    "train_size,valid_size,test_size = len(train_df),len(valid_df),len(test_df)\n",
    "print(train_size,valid_size,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object \n",
    "bert_model1=Bert_Model(train_df=train_df,\n",
    "                      test_df=test_df,\n",
    "                      bert_model_name=bert_model_name,\n",
    "                      bert_model_path=bert_model_path,\n",
    "                      tokenizer=tokenizer,\n",
    "                      max_seq_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %timeit\n",
    "# print(\"--Tokenizing--\")\n",
    "# train_data_tokenized,train_attention_mask = bert_model1.tokenize(train_data)\n",
    "# valid_data_tokenized,valid_attention_mask = bert_model1.tokenize(valid_data)\n",
    "# test_data_tokenized,test_attention_mask = bert_model1.tokenize(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to files for later use\n",
    "# TOKEN_DATA_PATH = '/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/tokenized/'\n",
    "# with open(TOKEN_DATA_PATH+'train_data_tokenized.txt', 'wb') as fp:\n",
    "#     pickle.dump(train_data_tokenized, fp)\n",
    "# with open(TOKEN_DATA_PATH+'train_attention_mask.txt', 'wb') as fp:\n",
    "#     pickle.dump(train_attention_mask, fp)    \n",
    "# with open(TOKEN_DATA_PATH+'valid_data_tokenized.txt', 'wb') as fp:\n",
    "#     pickle.dump(valid_data_tokenized, fp)\n",
    "# with open(TOKEN_DATA_PATH+'valid_attention_mask.txt', 'wb') as fp:\n",
    "#     pickle.dump(valid_attention_mask, fp)\n",
    "# with open(TOKEN_DATA_PATH+'test_data_tokenized.txt', 'wb') as fp:\n",
    "#     pickle.dump(test_data_tokenized, fp)\n",
    "# with open(TOKEN_DATA_PATH+'test_attention_mask.txt', 'wb') as fp:\n",
    "#     pickle.dump(test_attention_mask, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back tokenized data\n",
    "TOKEN_DATA_PATH = '/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/tokenized/'\n",
    "with open(TOKEN_DATA_PATH+'train_data_tokenized.txt', 'rb') as fp:\n",
    "    train_data_tokenized=pickle.load(fp)\n",
    "with open(TOKEN_DATA_PATH+'train_attention_mask.txt', 'rb') as fp:\n",
    "    train_attention_mask=pickle.load(fp)    \n",
    "with open(TOKEN_DATA_PATH+'valid_data_tokenized.txt', 'rb') as fp:\n",
    "    valid_data_tokenized=pickle.load(fp)\n",
    "with open(TOKEN_DATA_PATH+'valid_attention_mask.txt', 'rb') as fp:\n",
    "    valid_attention_mask=pickle.load(fp)\n",
    "with open(TOKEN_DATA_PATH+'test_data_tokenized.txt', 'rb') as fp:\n",
    "    test_data_tokenized=pickle.load(fp)\n",
    "with open(TOKEN_DATA_PATH+'test_attention_mask.txt', 'rb') as fp:\n",
    "    test_attention_mask=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(train_data_tokenized,train_attention_mask,train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_dataset = CreateDataset(valid_data_tokenized,valid_attention_mask,valid_labels)\n",
    "valid_sampler = RandomSampler(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated number of training batches:4284\n",
      "Generated number of validation batches:401\n",
      "Number of training steps: 4284\n",
      "Number of validation steps: 53.55\n",
      "Training summary steps: 214.0\n",
      "Validation summary steps: 54.0\n"
     ]
    }
   ],
   "source": [
    "# # Parameters for cloud GPU\n",
    "# train_params = {'batch_size': 48,\n",
    "#           'shuffle': False,\n",
    "#           'num_workers': 16}\n",
    "# valid_params = {'batch_size': 48,\n",
    "#           'shuffle': False,\n",
    "#           'num_workers': 16}\n",
    "\n",
    "# # Parameters for Suhas's Mac\n",
    "# train_params = {'batch_size': 128,\n",
    "#           'shuffle': False,\n",
    "#           'num_workers': 16}\n",
    "# valid_params = {'batch_size': 128,\n",
    "#           'shuffle': False,\n",
    "#           'num_workers': 16}\n",
    "\n",
    "# Parameters for local GPU\n",
    "train_params = {'batch_size': 48,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "valid_params = {'batch_size': 128,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "\n",
    "max_epochs = 1\n",
    "accumulation_steps=20\n",
    "\n",
    "# print(\"Expected number of batches:\", int(len(train_data_tokenized)/params['batch_size']))\n",
    "train_dataLoader = torch.utils.data.DataLoader(train_dataset,sampler=train_sampler,batch_size=train_params['batch_size'],\n",
    "                                         shuffle=train_params['shuffle'],\n",
    "                                         num_workers=train_params['num_workers'],\n",
    "                                         pin_memory=False,drop_last=True)\n",
    "valid_dataLoader = torch.utils.data.DataLoader(valid_dataset,sampler=valid_sampler,batch_size=valid_params['batch_size'],\n",
    "                                         shuffle=valid_params['shuffle'],\n",
    "                                         num_workers=valid_params['num_workers'],\n",
    "                                         pin_memory=False,drop_last=True)\n",
    "print(\"Generated number of training batches:%d\" %len(train_dataLoader))\n",
    "print(\"Generated number of validation batches:%d\" %len(valid_dataLoader))\n",
    "print(\"Number of training steps:\",len(train_dataLoader)*max_epochs)\n",
    "print(\"Number of validation steps:\",len(train_dataLoader)*max_epochs/(4*accumulation_steps))\n",
    "\n",
    "print(\"Training summary steps:\", round(len(train_dataLoader)*max_epochs/accumulation_steps,0))\n",
    "print(\"Validation summary steps:\", round(len(train_dataLoader)*max_epochs/accumulation_steps/4,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model,optimizer,scheduler,criterion,EPOCHS = bert_model1.initialize_model_for_training(len(train_dataLoader),\\\n",
    "                                                        EPOCHS=max_epochs,accumulation_steps=accumulation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add mpi run capability for distributed processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the pre-existing log directory\n",
    "logdir = './logs/1epoch_balanced/'\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990d6ab92ba649caac63b656a3bab0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1606), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab91b8441f940f7a675d6803a6c4194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:1||Train Loss:0.70826||Train Accuracy:0.49767\n",
      "Epoch Number:1||Valid Loss:0.00177||Valid Accuracy:0.00120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1606), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6dd06cb4644f30a6fb0869b49cce40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Training--\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:2||Train Loss:0.70692||Train Accuracy:0.49736\n",
      "Epoch Number:2||Valid Loss:0.00177||Valid Accuracy:0.00120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4284), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80569c814b742c2ab17f9f28cff59fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fed52a8cb214562adeb4d5a728a52e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04c50e7e07941cc901d8bcc6a7b65f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffaee76e7c044dabdb2062f29043cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "400\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cd4e5e99604f7c8b9583b83c73ab7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531dfc3aeda04709bc4d8e996d8448fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913cda295e94484595988ff196e7b18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6b89e36bf84e058c822b5cfc01ae13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6233a91c8f0041308c75571626ac1477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d157490f7237460fb23746d5fd5b28a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfac54f06b94908b31a80e5490eb1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167f614292f44546ae81c0c6bcb3de68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fefe62268eb4e8b9e473fd971925909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448586aff850486aa591034019d9aea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05106066933c48b59e57d03010085484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296f8584c58c49e5a41a3541abecd756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "2040\n",
      "2060\n",
      "2080\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b00ac07be8f453aa55bf7e0247673f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n",
      "2120\n",
      "2140\n",
      "2160\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778356467d044243b2786d7d2f9b0bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2660\n",
      "2680\n",
      "2700\n",
      "2720\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a2dbae005243768ac4673c6de2f967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d11423a89b4a59b3635d5f27e2ba17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cd3585712b4aa3a536a92ad974e1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3d14c171f94e119e8d8da055630f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3a708325cc4f7583e404a7da2fe6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5e8bf5c58f424da5e4624a45d09181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420964adc0924e8fb75326fc4c009572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27496a9515ba4125b7a01211944e8477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dce74107a9d40228b3d3b2ee555ef3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70382dbf7a64925a834c2d88f705216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcf956cfb154323bae4dcfa2de2f1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb6435ada1b41528defe3bfd7a9609e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77b6fb6461b41ae945b528ec02da69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb55a3665c7b46a7bbb5740f0c707577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fcb862dfe343ce9247491ddaafe9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46373c28b8094741932da12d01188086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bbd0c73cfb4237875ec409c5eaba80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474cc4de7a5242989e979324a5f8ba07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:3||Train Loss:0.70722||Train Accuracy:0.49735\n",
      "Epoch Number:3||Valid Loss:0.00177||Valid Accuracy:0.00120\n"
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900\n",
      "2920\n",
      "2940\n",
      "2960\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e888e6b043f480a8b115be21ba1a254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ec8990423f4d73bac3f018f028e7d5",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3540\n",
      "3560\n",
      "3580\n",
      "3600\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685f17c08b8a468083416998dfdec265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:4||Train Loss:0.70682||Train Accuracy:0.49737\n",
      "Epoch Number:4||Valid Loss:0.00177||Valid Accuracy:0.00120\n"
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d304496f0add4c9e88889525bcc6cc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0e4205a66040fc9710410d13cfeeb7",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n",
      "3720\n",
      "3740\n",
      "3760\n",
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c1ab2729424b029e642acbc0e6d052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac75463e2eef45fc84bb1b0a66685cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:5||Train Loss:0.70676||Train Accuracy:0.49736\n",
      "Epoch Number:5||Valid Loss:0.00176||Valid Accuracy:0.00120\n"
      "--I-- Running Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf12a65a0df459094d395d65e10caaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "trained_model=bert_model1.run_training(model,train_dataLoader,valid_dataLoader,\n",
    "                         optimizer=optimizer,scheduler=scheduler,EPOCHS=EPOCHS)"
    "\n",
    "start = time.time()\n",
    "trained_model,training_stats,validation_stats=\\\n",
    "        bert_model1.run_training(model,train_dataLoader,valid_dataLoader,\\\n",
    "        optimizer=optimizer,scheduler=scheduler,\\\n",
    "        criterion=criterion,EPOCHS=EPOCHS,\\\n",
    "        accumulation_steps=accumulation_steps,logdir=logdir)\n",
    "\n",
    "print(\"Training Time:%0.5f seconds\" %(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/pytorch-checkpoint.pt\"\n",
    "#  'optimizer_state_dict': optimizer.state_dict(),\n",
    "# 'scheduler_state_dict': scheduler.state_dict(),\n",
    "\n",
    "torch.save(trained_model,MODEL_PATH)\n"
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = [x['train_loss'] for x in training_stats]\n",
    "training_acc = [x['train_acc'] for x in training_stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model_from_checkpoint=Bert_Model(train_df=train_df,\n",
    "#                       test_df=test_df,\n",
    "#                       bert_model_name=bert_model_name,\n",
    "#                       bert_model_path=bert_model_path,\n",
    "#                       tokenizer=tokenizer,\n",
    "#                       max_seq_length=max_seq_len)\n",
    "\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# # # optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "# # checkpoint = torch.load(MODEL_PATH)\n",
    "# # bert_model_from_checkpoint.load_state_dict(checkpoint['model_state_dict'])\n",
    "# # # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# # # epoch = checkpoint['epoch']\n",
    "# # # loss = checkpoint['loss']\n",
    "\n",
    "# # bert_model_from_checkpoint.eval()\n",
    "# # # # - or -\n",
    "# # # model.train()"
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ac4f04531c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_losses' is not defined"
     ]
    }
   ],
   "source": [
    "training_losses\n",
    "training_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/pytorch-model-checkpoint-2epoch-suhas.bin\"\n",
    "MODEL_PATH = \"/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/pytorch-model.pt\"\n",
    "torch.save(trained_model.state_dict(),MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr /root/data/BERT/uncased_L-12_H-768_A-12-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export to ONNX format\n",
    "\n",
    "# model_pytorch.load_state_dict(torch.load('/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/pytorch-model.pt'))\n",
    "# torch.onnx.export(model_pytorch, dummy_input, './models/model_simple.onnx', input_names=['test_input'], output_names=['test_output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = '/root/data/BERT/trained-models-balanced_data-seq_128-epoch_2-lr_2e-5/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model = trained_model\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
