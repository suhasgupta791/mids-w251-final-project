{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "from apex import amp\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 7219\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:5879: ResourceWarning: unclosed file <_io.TextIOWrapper name='/root/.keras/keras.json' mode='r' encoding='ANSI_X3.4-1968'>\n",
      "  _config = json.load(open(_config_path))\n"
     ]
    }
   ],
   "source": [
    "# Import transformers specific packages\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import  BertForSequenceClassification, BertForTokenClassification\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device and empty cache \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export trained model for inference on TX2\n",
    "- Add tensorboard summaries for monitoring training \n",
    "- Add accuracy, F1 score, log loss metric calculations\n",
    "- Add architectural layers (freeze BERT layers, add new layers)\n",
    "- Create pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for model training and inference\n",
    "class Bert_Model():\n",
    "    def __init__(self,train_df,test_df,bert_model_name,bert_model_path,\n",
    "                tokenizer,\n",
    "                max_seq_length=128,seed=1234):\n",
    "        \n",
    "        if max_seq_length > tokenizer.max_model_input_sizes[bert_model_name]:\n",
    "            print(\"Max sequence length specified > 512!!... resetting to 128\")\n",
    "            print(\"If you don't want this then set max_seq_length to <= 512\")\n",
    "            self._MAX_SEQUENCE_LENGTH = 128\n",
    "        else:\n",
    "            self._MAX_SEQUENCE_LENGTH = max_seq_length\n",
    "        self._SEED = seed\n",
    "        self._WORK_DIR = \"/root/models/Tranformer_based/workingdir/\"\n",
    "        self._bert_model_path=bert_model_path\n",
    "        self._bert_model_name=bert_model_name\n",
    "        self._train_data=train_df\n",
    "        self._test_size=test_df\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self,text_array):\n",
    "        ''' Returns tokenized IDs and attention mask\n",
    "        The transformers encode_plus method returns the following:\n",
    "        {\n",
    "        input_ids: list[int],\n",
    "        token_type_ids: list[int] if return_token_type_ids is True (default)\n",
    "        attention_mask: list[int] if return_attention_mask is True (default)\n",
    "        overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "        num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "        special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "        }'''\n",
    "        all_tokens=[]\n",
    "        all_attention_mask=[]\n",
    "        for i,text in enumerate(tqdm_notebook(text_array)):\n",
    "            encoded = tokenizer.encode_plus(\n",
    "                           text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=self._MAX_SEQUENCE_LENGTH,\n",
    "                           pad_to_max_length=True)\n",
    "            tokens = torch.tensor(encoded['input_ids'])\n",
    "            attention_mask = torch.tensor(encoded['attention_mask'])\n",
    "            all_tokens.append(tokens)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "        return all_tokens,all_attention_mask\n",
    "     \n",
    "    def initialize_model_for_training(self,dataset_len,EPOCHS=1,model_seed=21000,lr=2e-5,batch_size=32,\n",
    "                                      accumulation_steps=2):\n",
    "        # Setup model parameters\n",
    "        np.random.seed(model_seed)\n",
    "        torch.manual_seed(model_seed)\n",
    "        torch.cuda.manual_seed(model_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "        model = BertForSequenceClassification.from_pretrained(self._bert_model_path,\n",
    "                                                              cache_dir=None,\n",
    "                                                              num_labels=2,\n",
    "                                                              output_attentions = False, \n",
    "                                                              output_hidden_states = False)\n",
    "        model.zero_grad()\n",
    "        model = model.to(device)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        num_train_optimization_steps = int(EPOCHS*dataset_len/batch_size/accumulation_steps)\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=1000,num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n",
    "        model, optimizer = amp.initialize(model,optimizer,opt_level=\"O1\",verbosity=0)\n",
    "        model=model.train()\n",
    "        return model,optimizer,scheduler,EPOCHS\n",
    "    \n",
    "    def run_training(self,model,train_dataLoader,valid_dataLoader,optimizer,scheduler,EPOCHS=1,batch_size=32,accumulation_steps=2):\n",
    "        tq = tqdm_notebook(range(EPOCHS))\n",
    "        for epoch in tq:\n",
    "            avg_loss = 0.\n",
    "            avg_accuracy = 0.\n",
    "            lossf=None\n",
    "            tk0 = tqdm_notebook(enumerate(train_dataLoader),total=len(train_dataLoader),leave=False)\n",
    "            optimizer.zero_grad()   \n",
    "            for step,(x_batch, attn_mask,y_batch) in tk0:\n",
    "                outputs = model(x_batch.to(device), \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=attn_mask.to(device), \n",
    "                                labels=y_batch.to(device))\n",
    "                loss, y_pred = outputs\n",
    "#                 loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n",
    "#                 with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#                     scaled_loss.backward()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip the norm to 1.0 to prevent exploding gradients\n",
    "                if (step+1) % accumulation_steps == 0:          # Wait for several backward steps\n",
    "                    model.zero_grad()\n",
    "                    optimizer.step()                            # Now we can do an optimizer step\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                if lossf:\n",
    "                    lossf = 0.98*lossf+0.02*loss.item()\n",
    "                else:\n",
    "                    lossf = loss.item()\n",
    "                tk0.set_postfix(loss = lossf)\n",
    "                avg_loss += lossf / len(train_dataLoader)\n",
    "                avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch>0.5).to(device)).to(torch.float) ).item()/len(train_dataLoader)\n",
    "            tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "            # Run evaluation\n",
    "            eval_loss,eval_accuracy=self.run_eval(model,valid_dataLoader,epoch)\n",
    "            print(\"Epoch Number:%d||Train Loss:%0.5f||Train Accuracy:%0.5f\" %(epoch+1,avg_loss,avg_accuracy))\n",
    "            print(\"Epoch Number:%d||Valid Loss:%0.5f||Valid Accuracy:%0.5f\" %(epoch+1,eval_loss,eval_accuracy))\n",
    "        return model\n",
    "    \n",
    "    # Function to calculate the accuracy of predictions vs labels\n",
    "    def flat_accuracy(self,preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    def run_eval(self,model,valid_dataLoader,epoch):\n",
    "        tq = tqdm_notebook(range(1))\n",
    "        avg_loss = 0.\n",
    "        eval_accuracy = 0.\n",
    "        eval_loss = 0.\n",
    "        nb_eval_steps = 0\n",
    "        lossf=None\n",
    "        tk0 = tqdm_notebook(enumerate(valid_dataLoader),total=len(valid_dataLoader),leave=False)\n",
    "        for step,(x_batch, attn_mask,y_batch) in tk0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(x_batch.to(device), \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=attn_mask.to(device), \n",
    "                                labels=y_batch.to(device))\n",
    "            loss, y_pred = outputs\n",
    "            y_pred = y_pred.detach().cpu().numpy()\n",
    "            label_ids = y_batch.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = self.flat_accuracy(y_pred, label_ids)\n",
    "            if lossf:\n",
    "                lossf = 0.98*lossf+0.02*loss.item()\n",
    "            else:\n",
    "                lossf = loss.item()\n",
    "            # Accumulate the total accuracy.\n",
    "            eval_loss += lossf/len(valid_dataLoader)\n",
    "            eval_accuracy += tmp_eval_accuracy/len(valid_dataLoader)\n",
    "            nb_eval_steps += 1\n",
    "        avg_loss = eval_loss/nb_eval_steps\n",
    "        avg_accuracy = eval_accuracy/nb_eval_steps\n",
    "        tq.set_postfix(avg_accuracy=avg_accuracy)\n",
    "        return avg_loss,avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self,data,atten_mask,labels):\n",
    "        self._dataset = [[data[i],atten_mask[i],labels.values[i]] for i in range(0,len(data))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self._dataset[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7999 2000 999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Define constants and paths\n",
    "\n",
    "seed = 7843\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "# Convert TF checkpoint to pytorch chckpoint and then use as input to class object\n",
    "bert_model_path = \"/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/\"\n",
    "# data_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/nlp.cs.princeton.edu/SARC/2.0/pol/\"\n",
    "data_path = \"/root/data/nlp.cs.princeton.edu/SARC/2.0/files/\"\n",
    "work_dir = \"/root/models/Tranformer_based/workingdir/\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, cache_dir=None,do_lower_case=True)\n",
    "max_seq_len = 128\n",
    "\n",
    "# Load and check the dataset from files on disk \n",
    "# train_file_name = data_path+'balanced_train.csv'\n",
    "# test_file_name  = data_path+'balanced_test.csv'\n",
    "\n",
    "train_file_name = data_path+'small_train.csv'\n",
    "test_file_name  = data_path+'small_test.csv'\n",
    "\n",
    "all_train_df = pd.read_csv(train_file_name)\n",
    "test_df = pd.read_csv(test_file_name)\n",
    "\n",
    "# Create a train, valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_train_df, test_size=0.2,random_state=seed)\n",
    "\n",
    "train_data   = train_df.text.fillna(\"DUMMY_VALUE\")\n",
    "train_labels = train_df.label\n",
    "valid_data  = valid_df.text.fillna(\"DUMMY_VALUE\")\n",
    "valid_labels = valid_df.label\n",
    "test_data  = test_df.text.fillna(\"DUMMY_VALUE\")\n",
    "test_labels = test_df.label\n",
    "\n",
    "train_size,valid_size,test_size = len(train_df),len(valid_df),len(test_df)\n",
    "print(train_size,valid_size,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object \n",
    "bert_model1=Bert_Model(train_df=train_df,\n",
    "                      test_df=test_df,\n",
    "                      bert_model_name=bert_model_name,\n",
    "                      bert_model_path=bert_model_path,\n",
    "                      tokenizer=tokenizer,\n",
    "                      max_seq_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3200c8b340f84a849f93bb8463b56cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c91350684de45b48a0b21129f8ec6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d494dfba6d04d079f87584fe9d62e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "train_data_tokenized,train_attention_mask = bert_model1.tokenize(train_data)\n",
    "valid_data_tokenized,valid_attention_mask = bert_model1.tokenize(valid_data)\n",
    "test_data_tokenized,test_attention_mask = bert_model1.tokenize(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(train_data_tokenized,train_attention_mask,train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_dataset = CreateDataset(valid_data_tokenized,valid_attention_mask,valid_labels)\n",
    "valid_sampler = RandomSampler(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_dataset[0]\n",
    "# valid_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_params = {'batch_size': 128,\\n          'shuffle': False,\\n          'num_workers': 16}\\nvalid_params = {'batch_size': 128,\\n          'shuffle': False,\\n          'num_workers': 16}\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated number of training batches:166\n",
      "Generated number of validation batches:7\n"
     ]
    }
   ],
   "source": [
    "# Parameters for cloud GPU\n",
    "'''\n",
    "train_params = {'batch_size': 128,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "valid_params = {'batch_size': 128,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "'''\n",
    "# Parameters for local GPU\n",
    "train_params = {'batch_size': 48,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "valid_params = {'batch_size': 256,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16}\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "# print(\"Expected number of batches:\", int(len(train_data_tokenized)/params['batch_size']))\n",
    "train_dataLoader = torch.utils.data.DataLoader(train_dataset,sampler=train_sampler,batch_size=train_params['batch_size'],\n",
    "                                         shuffle=train_params['shuffle'],\n",
    "                                         num_workers=train_params['num_workers'],\n",
    "                                         pin_memory=False,drop_last=True)\n",
    "valid_dataLoader = torch.utils.data.DataLoader(valid_dataset,sampler=valid_sampler,batch_size=valid_params['batch_size'],\n",
    "                                         shuffle=valid_params['shuffle'],\n",
    "                                         num_workers=valid_params['num_workers'],\n",
    "                                         pin_memory=False,drop_last=True)\n",
    "print(\"Generated number of training batches:%d\" %len(train_dataLoader))\n",
    "print(\"Generated number of validation batches:%d\" %len(valid_dataLoader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model,optimizer,scheduler,EPOCHS = bert_model1.initialize_model_for_training(len(train_dataLoader),\n",
    "                                                                             EPOCHS=max_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add mpi run capability for distributed processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3c4c1ec8bd4a63a2342237ffce3e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=166), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b69b37e7a19487780771b33e2784566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37829fa4feec44428fa5128aaa420885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train \n",
    "\n",
    "start = time.time()\n",
    "trained_model=bert_model1.run_training(model,train_dataLoader,valid_dataLoader,\n",
    "                         optimizer=optimizer,scheduler=scheduler,EPOCHS=EPOCHS)\n",
    "\n",
    "print(\"Training Time:\", time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
