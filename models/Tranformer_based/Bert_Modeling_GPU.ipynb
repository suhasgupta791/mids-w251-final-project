{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "from apex import amp\n",
    "import shutil\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 7219\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:5879: ResourceWarning: unclosed file <_io.TextIOWrapper name='/root/.keras/keras.json' mode='r' encoding='ANSI_X3.4-1968'>\n",
      "  _config = json.load(open(_config_path))\n"
     ]
    }
   ],
   "source": [
    "# Import transformers specific packages\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import  BertForSequenceClassification, BertForTokenClassification\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device and empty cache \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export trained model for inference on TX2\n",
    "- Add tensorboard summaries for monitoring training \n",
    "- Add accuracy, F1 score, log loss metric calculations\n",
    "- Add eval steps\n",
    "- Add architectural layers (freeze BERT layers, add new layers)\n",
    "- Create pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for model training and inference\n",
    "class Bert_Model():\n",
    "    def __init__(self,train_df,test_df,bert_model_name,bert_model_path,\n",
    "                tokenizer,\n",
    "                max_seq_length=128,seed=1234):\n",
    "        \n",
    "        if max_seq_length > tokenizer.max_model_input_sizes[bert_model_name]:\n",
    "            print(\"Max sequence length specified > 512!!... resetting to 128\")\n",
    "            print(\"If you don't want this then set max_seq_length to <= 512\")\n",
    "            self._MAX_SEQUENCE_LENGTH = 128\n",
    "        else:\n",
    "            self._MAX_SEQUENCE_LENGTH = max_seq_length\n",
    "        self._SEED = seed\n",
    "        self._WORK_DIR = \"/Users/suhasgupta/w251/mids-w251-final-project/models/Tranformer_based/workingdir/\"\n",
    "        self._bert_model_path=bert_model_path\n",
    "        self._bert_model_name=bert_model_name\n",
    "        self._train_data=train_df\n",
    "        self._test_size=test_df\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self,text_array):\n",
    "        ''' Returns tokenized IDs and attention mask\n",
    "        The transformers encode_plus method returns the following:\n",
    "        {\n",
    "        input_ids: list[int],\n",
    "        token_type_ids: list[int] if return_token_type_ids is True (default)\n",
    "        attention_mask: list[int] if return_attention_mask is True (default)\n",
    "        overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "        num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "        special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "        }'''\n",
    "        all_tokens=[]\n",
    "        all_attention_mask=[]\n",
    "        for i,text in enumerate(tqdm_notebook(text_array)):\n",
    "            encoded = tokenizer.encode_plus(\n",
    "                           text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=self._MAX_SEQUENCE_LENGTH,\n",
    "                           pad_to_max_length=True)\n",
    "            tokens = torch.tensor(encoded['input_ids'])\n",
    "            attention_mask = torch.tensor(encoded['attention_mask'])\n",
    "            all_tokens.append(tokens)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "        return all_tokens,all_attention_mask\n",
    "     \n",
    "    def initialize_model_for_training(self,dataset_len,EPOCHS=1,model_seed=21000,lr=2e-5,batch_size=32,\n",
    "                                      accumulation_steps=2):\n",
    "        # Setup model parameters\n",
    "        np.random.seed(model_seed)\n",
    "        torch.manual_seed(model_seed)\n",
    "        torch.cuda.manual_seed(model_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "        model = BertForSequenceClassification.from_pretrained(self._bert_model_path,\n",
    "                                                              cache_dir=None,\n",
    "                                                              num_labels=2,\n",
    "                                                              output_attentions = False, \n",
    "                                                              output_hidden_states = False)\n",
    "        model.zero_grad()\n",
    "        model = model.to(device)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        num_train_optimization_steps = int(EPOCHS*dataset_len/batch_size/accumulation_steps)\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=1000,num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n",
    "        model, optimizer = amp.initialize(model,optimizer,opt_level=\"O1\",verbosity=0)\n",
    "        model=model.train()\n",
    "        return model,optimizer,scheduler,EPOCHS\n",
    "    \n",
    "    def run_training(self,model,train_dataLoader,optimizer,scheduler,EPOCHS=1,batch_size=32,accumulation_steps=2):\n",
    "        tq = tqdm_notebook(range(EPOCHS))\n",
    "        for epoch in tq:\n",
    "            avg_loss = 0.\n",
    "            avg_accuracy = 0.\n",
    "            lossf=None\n",
    "            tk0 = tqdm_notebook(enumerate(train_dataLoader),total=len(train_dataLoader),leave=False)\n",
    "            optimizer.zero_grad()   \n",
    "            model.zero_grad()\n",
    "            for step,(x_batch, attn_mask,y_batch) in tk0:\n",
    "                outputs = model(x_batch.to(device), \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=attn_mask.to(device), \n",
    "                                labels=y_batch.to(device))\n",
    "                loss, y_pred = outputs\n",
    "#                 loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n",
    "#                 with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#                     scaled_loss.backward()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip the norm to 1.0 to prevent exploding gradients\n",
    "                if (step+1) % accumulation_steps == 0:          # Wait for several backward steps\n",
    "                    model.zero_grad()\n",
    "                    optimizer.step()                            # Now we can do an optimizer step\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                if lossf:\n",
    "                    lossf = 0.98*lossf+0.02*loss.item()\n",
    "                else:\n",
    "                    lossf = loss.item()\n",
    "                tk0.set_postfix(loss = lossf)\n",
    "                avg_loss += loss.item() / len(train_dataLoader)\n",
    "                avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch>0.5).to(device)).to(torch.float) ).item()/len(train_dataLoader)\n",
    "            tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "            print(\"Epoch Number:\",epoch,\"||Epoch Loss:\",avg_loss,\"|Epoch Accuracy:\",avg_accuracy)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self,data,atten_mask,labels):\n",
    "        self._dataset = [[data[i],atten_mask[i],labels.values[i]] for i in range(0,len(data))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self._dataset[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205665 51417 64666\n"
     ]
    }
   ],
   "source": [
    "# Define constants and paths\n",
    "\n",
    "seed = 7843\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "# Convert TF checkpoint to pytorch chckpoint and then use as input to class object\n",
    "bert_model_path = \"/root/data/BERT/uncased_L-12_H-768_A-12-pytorch/\"\n",
    "# data_path = \"/Users/suhasgupta/w251/mids-w251-final-project/data/nlp.cs.princeton.edu/SARC/2.0/pol/\"\n",
    "data_path = \"/root/data/nlp.cs.princeton.edu/SARC/2.0/files/\"\n",
    "work_dir = \"/root/models/Tranformer_based/workingdir/\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, cache_dir=None,do_lower_case=True)\n",
    "max_seq_len = 128\n",
    "\n",
    "# Load and check the dataset from files on disk \n",
    "train_file_name = data_path+'balanced_train.csv'\n",
    "test_file_name  = data_path+'balanced_test.csv'\n",
    "\n",
    "all_train_df = pd.read_csv(train_file_name)\n",
    "test_df = pd.read_csv(test_file_name)\n",
    "\n",
    "# Create a train, valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_train_df, test_size=0.2,random_state=seed)\n",
    "\n",
    "train_data   = train_df.text.fillna(\"DUMMY_VALUE\")\n",
    "train_labels = train_df.label\n",
    "valid_data  = valid_df.text.fillna(\"DUMMY_VALUE\")\n",
    "valid_labels = valid_df.label\n",
    "test_data  = test_df.text.fillna(\"DUMMY_VALUE\")\n",
    "test_labels = test_df.label\n",
    "\n",
    "train_size,valid_size,test_size = len(train_df),len(valid_df),len(test_df)\n",
    "print(train_size,valid_size,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object \n",
    "bert_model1=Bert_Model(train_df=train_df,\n",
    "                      test_df=test_df,\n",
    "                      bert_model_name=bert_model_name,\n",
    "                      bert_model_path=bert_model_path,\n",
    "                      tokenizer=tokenizer,\n",
    "                      max_seq_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e413b78242e640d99c60caf7aa0b129b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=205665), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee8006c45364c6e92842150436a5da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51417), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29159021cf1d4014877c42f823cbdea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64666), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "train_data_tokenized,train_attention_mask = bert_model1.tokenize(train_data)\n",
    "valid_data_tokenized,valid_attention_mask = bert_model1.tokenize(valid_data)\n",
    "test_data_tokenized,test_attention_mask = bert_model1.tokenize(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(train_data_tokenized,train_attention_mask,train_labels)\n",
    "valid_dataset = CreateDataset(valid_data_tokenized,valid_attention_mask,valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Figure out why the train and valid are the same!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  101,  2106,  2017,  9530,  8873, 27390,  2063,  9779, 11140,  3512,\n",
       "         19528,  3370,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[tensor([  101,  2009,  2064,  1005,  1056,  2022,  1010,  2296,  2309,  2865,\n",
       "          3029,  2003,  3424,  3366, 22930,  2594,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]\n",
    "valid_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of batches: 1606\n",
      "Generated number of batches:1606\n"
     ]
    }
   ],
   "source": [
    "params = {'batch_size': 128,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 16,\n",
    "          'max_epochs':10}\n",
    "print(\"Expected number of batches:\", int(len(train_data_tokenized)/params['batch_size']))\n",
    "train_dataLoader = torch.utils.data.DataLoader(train_dataset,batch_size=params['batch_size'],\n",
    "                                         shuffle=params['shuffle'],\n",
    "                                         num_workers=params['num_workers'],\n",
    "                                         pin_memory=False,drop_last=True)\n",
    "print(\"Generated number of batches:%d\" %len(train_dataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2106,  2017,  ...,     0,     0,     0],\n",
      "        [  101,  6616,  3398,  ...,     0,     0,     0],\n",
      "        [  101,  2065,  2017,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2138,  5766,  ...,     0,     0,     0],\n",
      "        [  101, 26471,  2695,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2196,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for step,batch in enumerate(train_dataLoader):\n",
    "    print(batch[0])\n",
    "    print(batch[1])\n",
    "    print(batch[2])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model,optimizer,scheduler,EPOCHS = bert_model1.initialize_model_for_training(len(train_dataLoader),\n",
    "                                                                             EPOCHS=params['max_epochs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add mpi run capability for distributed processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb1a1dcf5954e64aa2e1ffde40b8f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31481ef31cda4a4c947519b2b2228f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1606), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train \n",
    "tq,model=bert_model1.run_training(model,train_dataLoader,\n",
    "                         optimizer=optimizer,scheduler=scheduler,EPOCHS=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
